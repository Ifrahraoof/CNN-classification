{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '3'\n",
    "\n",
    "# Silences the warning and error logs generated by TensorFlow\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE = 'C:/Users/USER/Superdataset/model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.compat.v1.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv('C:\\\\Users\\\\USER\\\\Superdataset\\\\Training_data.csv', header=None)\n",
    "train_data = np.array(train_data).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = pd.read_csv( 'C:\\\\Users\\\\USER\\\\Superdataset\\\\Training_labels.csv', header=None)\n",
    "train_labels = np.array(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('C:\\\\Users\\\\USER\\\\Superdataset\\\\Test_data.csv', header=None)\n",
    "test_data = np.array(test_data).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels = pd.read_csv('C:\\\\Users\\\\USER\\\\Superdataset\\\\Test_labels.csv', header=None)\n",
    "test_labels = np.array(test_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "n_batch = train_data.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pip-autoremoveNote: you may need to restart the kernel to use updated packages.\n",
      "  Downloading pip_autoremove-0.9.1-py2.py3-none-any.whl (3.8 kB)\n",
      "Installing collected packages: pip-autoremove\n",
      "Successfully installed pip-autoremove-0.9.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pip install pip-autoremove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Usage:   \n",
      "  C:\\Users\\USER\\anaconda3\\python.exe -m pip <command> [options]\n",
      "\n",
      "no such option: -a\n"
     ]
    }
   ],
   "source": [
    "pip-autoremove pandas -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.1.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (1.19.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2020.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "size = train_data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11128320"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(17388, 640)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ndim = train_data.ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ndim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (64, 1) for Tensor 'Input_7/Labels/Placeholder:0', which has shape '(None, 4)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-34-8ca703cf45d3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    547\u001b[0m         \u001b[0mbatch_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    548\u001b[0m         \u001b[0mbatch_ys\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 549\u001b[1;33m         \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mbatch_ys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;36m0.50\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    550\u001b[0m     \u001b[0mbatch_xs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch_xs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m64\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m32\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    551\u001b[0m     \u001b[1;31m# Accuracy on Training Set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    965\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    966\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 967\u001b[1;33m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0m\u001b[0;32m    968\u001b[0m                          run_metadata_ptr)\n\u001b[0;32m    969\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1162\u001b[0m           if (not is_tensor_handle_feed and\n\u001b[0;32m   1163\u001b[0m               not subfeed_t.get_shape().is_compatible_with(np_val.shape)):\n\u001b[1;32m-> 1164\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m   1165\u001b[0m                 \u001b[1;34m'Cannot feed value of shape %r for Tensor %r, '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m                 \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (64, 1) for Tensor 'Input_7/Labels/Placeholder:0', which has shape '(None, 4)'"
     ]
    }
   ],
   "source": [
    "\n",
    "sess = tf.InteractiveSession()\n",
    "\n",
    "# Read Training Data\n",
    "train_data = pd.read_csv('C:\\\\Users\\\\USER\\\\Superdataset\\\\Training_data.csv', header=None)\n",
    "train_data = np.array(train_data).astype('float32')\n",
    "\n",
    "# Read Training Labels\n",
    "train_labels = pd.read_csv('C:\\\\Users\\\\USER\\\\Superdataset\\\\Training_labels.csv', header=None)\n",
    "train_labels = np.array(train_labels)\n",
    "\n",
    "# Read Testing Data\n",
    "test_data = pd.read_csv('C:\\\\Users\\\\USER\\\\Superdataset\\\\Test_data.csv', header=None)\n",
    "test_data = np.array(test_data).astype('float32')\n",
    "\n",
    "# Read Testing Labels\n",
    "test_labels = pd.read_csv('C:\\\\Users\\\\USER\\\\Superdataset\\\\Test_labels.csv', header=None)\n",
    "test_labels = np.array(test_labels)\n",
    "\n",
    "# Set Batch Size 64\n",
    "batch_size = 64\n",
    "n_batch = train_data.shape[0] // batch_size\n",
    "\n",
    "# Initialize the Weights\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.01)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Initialize the Bias\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.01, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Define the Function of Summary\n",
    "def variable_summaries(var):\n",
    "    with tf.name_scope('summaries'):\n",
    "        mean = tf.reduce_mean(var)\n",
    "        tf.summary.scalar('mean', mean)\n",
    "\n",
    "        with tf.name_scope('stddev'):\n",
    "            stddev = tf.sqrt(tf.reduce_mean(tf.square(var - mean)))\n",
    "            tf.summary.scalar('stddev', stddev)\n",
    "\n",
    "        tf.summary.scalar('max', tf.reduce_max(var))\n",
    "        tf.summary.scalar('min', tf.reduce_min(var))\n",
    "        tf.summary.histogram('histogram', var)\n",
    "\n",
    "# Define Placeholders\n",
    "with tf.name_scope(\"Input\"):\n",
    "    # x is the input feature data\n",
    "    with tf.name_scope(\"Input_Data\"):\n",
    "        x = tf.placeholder(tf.float32, [None, 640])\n",
    "\n",
    "    # y is the label related to the data\n",
    "    with tf.name_scope(\"Labels\"):\n",
    "        y = tf.placeholder(tf.float32, [None, 4])\n",
    "\n",
    "    # Keep_Prob is the possibility that keep neural while using dropout\n",
    "    with tf.name_scope(\"Keep_Prob\"):\n",
    "        keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "    # Reshape the input data into 2-dimensional\n",
    "    with tf.name_scope(\"Reshape_Data\"):\n",
    "        x_Reshape = tf.reshape(tensor=x, shape=[-1, 32, 20, 1])\n",
    "    \n",
    "# First Convolutional Layer\n",
    "with tf.name_scope('Convolutional_1'):\n",
    "    with tf.name_scope('W_conv1'):\n",
    "        W_conv1 = weight_variable([3, 3, 1, 32])\n",
    "        # variable_summaries(W_conv1)\n",
    "\n",
    "    with tf.name_scope('b_conv1'):\n",
    "        b_conv1 = bias_variable([32])\n",
    "        # variable_summaries(b_conv1)\n",
    "\n",
    "    with tf.name_scope('h_conv1'):\n",
    "        h_conv1 = tf.nn.conv2d(x_Reshape, W_conv1, strides=[1, 1, 1, 1], padding='SAME') + b_conv1\n",
    "        # variable_summaries(h_conv1)\n",
    "\n",
    "    with tf.name_scope('h_conv1_Acti'):\n",
    "        h_conv1_Acti = tf.nn.leaky_relu(h_conv1)\n",
    "        # variable_summaries(h_conv1_Acti)\n",
    "\n",
    "    with tf.name_scope('h_conv1_drop'):\n",
    "        h_conv1_drop = tf.nn.dropout(h_conv1_Acti, keep_prob, noise_shape=[tf.shape(h_conv1_Acti)[0], 1, 1, tf.shape(h_conv1_Acti)[3]])\n",
    "        # variable_summaries(h_conv1_drop)\n",
    "\n",
    "# Second Convolutional Layer\n",
    "with tf.name_scope('Convolutional_2'):\n",
    "    with tf.name_scope('W_conv2'):\n",
    "        W_conv2 = weight_variable([3, 3, 32, 32])\n",
    "        # variable_summaries(W_conv2)\n",
    "\n",
    "    with tf.name_scope('b_conv2'):\n",
    "        b_conv2 = bias_variable([32])\n",
    "        # variable_summaries(b_conv2)\n",
    "\n",
    "    with tf.name_scope('h_conv2'):\n",
    "        h_conv2 = tf.nn.conv2d(h_conv1_drop, W_conv2, strides=[1, 1, 1, 1], padding='SAME') + b_conv2\n",
    "        # variable_summaries(h_conv2)\n",
    "\n",
    "    with tf.name_scope('h_conv2_BN'):\n",
    "        h_conv2_BN = tf.layers.batch_normalization(h_conv2, training=True)\n",
    "        # variable_summaries(h_conv2_BN)\n",
    "\n",
    "    with tf.name_scope('h_conv2_Acti'):\n",
    "        h_conv2_Acti = tf.nn.leaky_relu(h_conv2_BN)\n",
    "        # variable_summaries(h_conv2_Acti)\n",
    "\n",
    "# Third Convolutional Layer\n",
    "with tf.name_scope('Convolutional_3'):\n",
    "    with tf.name_scope('W_conv3'):\n",
    "        W_conv3 = weight_variable([3, 3, 64, 64])\n",
    "        # variable_summaries(W_conv3)\n",
    "\n",
    "    with tf.name_scope('b_conv3'):\n",
    "        b_conv3 = bias_variable([64])\n",
    "        # variable_summaries(b_conv3)\n",
    "\n",
    "    with tf.name_scope('h_conv3_res'):\n",
    "        h_conv3_res = tf.concat([h_conv2_Acti, h_conv1_drop], axis=3)\n",
    "        # variable_summaries(h_conv3_res)\n",
    "\n",
    "    with tf.name_scope('h_conv3'):\n",
    "        h_conv3 = tf.nn.conv2d(h_conv3_res, W_conv3, strides=[1, 1, 1, 1], padding='SAME') + b_conv3\n",
    "        # variable_summaries(h_conv3)\n",
    "\n",
    "    with tf.name_scope('h_conv3_Acti'):\n",
    "        h_conv3_Acti = tf.nn.leaky_relu(h_conv3)\n",
    "        # variable_summaries(h_conv3_Acti)\n",
    "\n",
    "    with tf.name_scope('h_pool3_drop'):\n",
    "        h_conv3_drop = tf.nn.dropout(h_conv3_Acti, keep_prob, noise_shape=[tf.shape(h_conv3_Acti)[0], 1, 1, tf.shape(h_conv3_Acti)[3]])\n",
    "        # variable_summaries(h_conv3_drop)\n",
    "\n",
    "# First Max Pooling Layer\n",
    "with tf.name_scope('Pooling_1'):\n",
    "    with tf.name_scope('h_pool3'):\n",
    "        h_pool3 = tf.nn.max_pool(h_conv3_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # variable_summaries(h_pool3)\n",
    "\n",
    "# Fourth Convolutional Layer\n",
    "with tf.name_scope('Convolutional_4'):\n",
    "    with tf.name_scope('W_conv4'):\n",
    "        W_conv4 = weight_variable([3, 3, 64, 64])\n",
    "        # variable_summaries(W_conv4)\n",
    "\n",
    "    with tf.name_scope('b_conv4'):\n",
    "        b_conv4 = bias_variable([64])\n",
    "        # variable_summaries(b_conv4)\n",
    "\n",
    "    with tf.name_scope('h_conv4'):\n",
    "        h_conv4 = tf.nn.conv2d(h_pool3, W_conv4, strides=[1, 1, 1, 1], padding='VALID') + b_conv4\n",
    "        # variable_summaries(h_conv4)\n",
    "\n",
    "    with tf.name_scope('h_conv4_BN'):\n",
    "        h_conv4_BN = tf.layers.batch_normalization(h_conv4, training=True)\n",
    "        # variable_summaries(h_conv4_BN)\n",
    "\n",
    "    with tf.name_scope('h_conv4_Acti'):\n",
    "        h_conv4_Acti = tf.nn.leaky_relu(h_conv4_BN)\n",
    "        # variable_summaries(h_conv4_Acti)\n",
    "\n",
    "    with tf.name_scope('h_conv4_drop'):\n",
    "        h_conv4_drop = tf.nn.dropout(h_conv4_Acti, keep_prob, noise_shape=[tf.shape(h_conv4_Acti)[0], 1, 1, tf.shape(h_conv4_Acti)[3]])\n",
    "        # variable_summaries(h_conv4_drop)\n",
    "\n",
    "# Fifth Convolutional Layer\n",
    "with tf.name_scope('Convolutional_5'):\n",
    "    with tf.name_scope('W_conv5'):\n",
    "        W_conv5 = weight_variable([3, 3, 64, 64])\n",
    "        # variable_summaries(W_conv5)\n",
    "\n",
    "    with tf.name_scope('b_conv5'):\n",
    "        b_conv5 = bias_variable([64])\n",
    "        # variable_summaries(b_conv5)\n",
    "\n",
    "    with tf.name_scope('h_conv5'):\n",
    "        h_conv5 = tf.nn.conv2d(h_conv4_drop, W_conv5, strides=[1, 1, 1, 1], padding='SAME') + b_conv5\n",
    "        # variable_summaries(h_conv5)\n",
    "\n",
    "    with tf.name_scope('h_conv5_BN'):\n",
    "        h_conv5_BN = tf.layers.batch_normalization(h_conv5, training=True)\n",
    "        # variable_summaries(h_conv5_BN)\n",
    "\n",
    "    with tf.name_scope('h_conv5_Acti'):\n",
    "        h_conv5_Acti = tf.nn.leaky_relu(h_conv5_BN)\n",
    "        # variable_summaries(h_conv5_Acti)\n",
    "\n",
    "# Sixth Convolutional Layer\n",
    "with tf.name_scope('Convolutional_6'):\n",
    "    with tf.name_scope('W_conv6'):\n",
    "        W_conv6 = weight_variable([3, 3, 128, 128])\n",
    "        # variable_summaries(W_conv6)\n",
    "\n",
    "    with tf.name_scope('b_conv6'):\n",
    "        b_conv6 = bias_variable([128])\n",
    "        # variable_summaries(b_conv6)\n",
    "\n",
    "    with tf.name_scope('h_conv6_res'):\n",
    "        h_conv6_res = tf.concat([h_conv5_Acti, h_conv4_drop], axis=3)\n",
    "        # variable_summaries(h_conv6_res)\n",
    "\n",
    "    with tf.name_scope('h_conv6'):\n",
    "        h_conv6 = tf.nn.conv2d(h_conv6_res, W_conv6, strides=[1, 1, 1, 1], padding='SAME') + b_conv6\n",
    "        # variable_summaries(h_conv6)\n",
    "\n",
    "    with tf.name_scope('h_conv6_Activation'):\n",
    "        h_conv6_Acti = tf.nn.leaky_relu(h_conv6)\n",
    "        # variable_summaries(h_conv6_Acti)\n",
    "\n",
    "    with tf.name_scope('h_pool6_drop'):\n",
    "        h_conv6_drop = tf.nn.dropout(h_conv6_Acti, keep_prob, noise_shape=[tf.shape(h_conv6_Acti)[0], 1, 1, tf.shape(h_conv6_Acti)[3]])\n",
    "        # variable_summaries(h_conv6_drop)\n",
    "\n",
    "# Second Max Pooling Layer\n",
    "with tf.name_scope('Pooling_2'):\n",
    "    with tf.name_scope('h_pool6'):\n",
    "        h_pool6 = tf.nn.max_pool(h_conv6_drop, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "        # variable_summaries(h_pool6)\n",
    "\n",
    "# Flatten Layer\n",
    "with tf.name_scope('Flatten'):\n",
    "    with tf.name_scope('h_pool6_flat'):\n",
    "        h_pool6_flat = tf.reshape(h_pool6, [-1, 4 * 7 * 128])\n",
    "        # variable_summaries(h_pool6_flat)\n",
    "\n",
    "# First Fully Connected Layer\n",
    "with tf.name_scope('Fully_Connected_1'):\n",
    "    with tf.name_scope('W_fc1'):\n",
    "        W_fc1 = weight_variable([4 * 7 * 128, 512])\n",
    "        # variable_summaries(W_fc1)\n",
    "\n",
    "    with tf.name_scope('b_fc1'):\n",
    "        b_fc1 = bias_variable([512])\n",
    "        # variable_summaries(b_fc1)\n",
    "\n",
    "    with tf.name_scope('h_fc1'):\n",
    "        h_fc1 = tf.matmul(h_pool6_flat, W_fc1) + b_fc1\n",
    "        # variable_summaries(h_fc1)\n",
    "\n",
    "    with tf.name_scope('h_fc1_BN'):\n",
    "        h_fc1_BN = tf.layers.batch_normalization(h_fc1, training=True)\n",
    "        # variable_summaries(h_fc1_BN)\n",
    "\n",
    "    with tf.name_scope('h_fc1_Acti'):\n",
    "        h_fc1_Acti = tf.nn.leaky_relu(h_fc1_BN)\n",
    "        # variable_summaries(h_fc1_Acti)\n",
    "\n",
    "    with tf.name_scope('h_fc1_drop'):\n",
    "        h_fc1_drop = tf.nn.dropout(h_fc1_Acti, keep_prob)\n",
    "        # variable_summaries(h_fc1_drop)\n",
    "\n",
    "# Second Fully Connected Layer\n",
    "with tf.name_scope('Output_Layer'):\n",
    "    with tf.name_scope('W_fc2'):\n",
    "        W_fc2 = weight_variable([512, 4])\n",
    "        # variable_summaries(W_fc2)\n",
    "\n",
    "    with tf.name_scope('b_fc2'):\n",
    "        b_fc2 = bias_variable([4])\n",
    "        # variable_summaries(b_fc2)\n",
    "\n",
    "    with tf.name_scope('prediction'):\n",
    "        prediction = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)\n",
    "        # variable_summaries(prediction)\n",
    "\n",
    "# Define Loss Function\n",
    "with tf.name_scope('loss'):\n",
    "    with tf.name_scope('Euclidean_Distance'):\n",
    "        loss = tf.reduce_mean(tf.square(y - prediction))\n",
    "        tf.summary.scalar('loss', loss)\n",
    "\n",
    "# Define Training Optimizer\n",
    "with tf.name_scope('Train_Optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-5).minimize(loss)\n",
    "\n",
    "# Calculate Accuracy\n",
    "# Add metrics to TensorBoard.\n",
    "with tf.name_scope('Evalution'):\n",
    "    # Calculate Each Task Accuracy\n",
    "    with tf.name_scope('Each_Class_accuracy'):\n",
    "        # Task 1 Accuracy\n",
    "        with tf.name_scope('T1_accuracy'):\n",
    "            # Number of Classified Correctly\n",
    "            y_T1 = tf.equal(tf.argmax(y, 1), 0)\n",
    "            prediction_T1 = tf.equal(tf.argmax(prediction, 1), 0)\n",
    "            T1_Corrected_Num = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T1, prediction_T1), tf.float32))\n",
    "\n",
    "            # Number of All the Test Samples\n",
    "            T1_all_Num = tf.reduce_sum(tf.cast(y_T1, tf.float32))\n",
    "\n",
    "            # Task 1 Accuracy\n",
    "            T1_accuracy = tf.divide(T1_Corrected_Num, T1_all_Num)\n",
    "            tf.summary.scalar('T1_accuracy', T1_accuracy)\n",
    "\n",
    "            T1_TP = T1_Corrected_Num\n",
    "            T1_TN = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T1), tf.math.logical_not(prediction_T1)), tf.float32))\n",
    "            T1_FP = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T1), prediction_T1), tf.float32))\n",
    "            T1_FN = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T1, tf.math.logical_not(prediction_T1)), tf.float32))\n",
    "\n",
    "            with tf.name_scope(\"T1_Precision\"):\n",
    "                T1_Precision = T1_TP / (T1_TP + T1_FP)\n",
    "                tf.summary.scalar('T1_Precision', T1_Precision)\n",
    "\n",
    "            with tf.name_scope(\"T1_Recall\"):\n",
    "                T1_Recall = T1_TP / (T1_TP + T1_FN)\n",
    "                tf.summary.scalar('T1_Recall', T1_Recall)\n",
    "\n",
    "            with tf.name_scope(\"T1_F_Score\"):\n",
    "                T1_F_Score = (2*T1_Precision*T1_Recall)/(T1_Precision+T1_Recall)\n",
    "                tf.summary.scalar('T1_F_Score', T1_F_Score)\n",
    "\n",
    "        # Task 2 Accuracy\n",
    "        with tf.name_scope('T2_accuracy'):\n",
    "            # Number of Classified Correctly\n",
    "            y_T2 = tf.equal(tf.argmax(y, 1), 1)\n",
    "            prediction_T2 = tf.equal(tf.argmax(prediction, 1), 1)\n",
    "            T2_Corrected_Num = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T2, prediction_T2), tf.float32))\n",
    "\n",
    "            # Number of All the Test Samples\n",
    "            T2_all_Num = tf.reduce_sum(tf.cast(y_T2, tf.float32))\n",
    "\n",
    "            # Task 2 Accuracy\n",
    "            T2_accuracy = tf.divide(T2_Corrected_Num, T2_all_Num)\n",
    "            tf.summary.scalar('T2_accuracy', T2_accuracy)\n",
    "\n",
    "            T2_TP = T2_Corrected_Num\n",
    "            T2_TN = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T2), tf.math.logical_not(prediction_T2)), tf.float32))\n",
    "            T2_FP = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T2), prediction_T2), tf.float32))\n",
    "            T2_FN = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T2, tf.math.logical_not(prediction_T2)), tf.float32))\n",
    "\n",
    "            with tf.name_scope(\"T2_Precision\"):\n",
    "                T2_Precision = T2_TP / (T2_TP + T2_FP)\n",
    "                tf.summary.scalar('T2_Precision', T2_Precision)\n",
    "\n",
    "            with tf.name_scope(\"T2_Recall\"):\n",
    "                T2_Recall = T2_TP / (T2_TP + T2_FN)\n",
    "                tf.summary.scalar('T2_Recall', T2_Recall)\n",
    "\n",
    "            with tf.name_scope(\"T2_F_Score\"):\n",
    "                T2_F_Score = (2*T2_Precision*T2_Recall)/(T2_Precision+T2_Recall)\n",
    "                tf.summary.scalar('T2_F_Score', T2_F_Score)\n",
    "\n",
    "        # Task 3 Accuracy\n",
    "        with tf.name_scope('T3_accuracy'):\n",
    "            # Number of Classified Correctly\n",
    "            y_T3 = tf.equal(tf.argmax(y, 1), 2)\n",
    "            prediction_T3 = tf.equal(tf.argmax(prediction, 1), 2)\n",
    "            T3_Corrected_Num = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T3, prediction_T3), tf.float32))\n",
    "\n",
    "            # Number of All the Test Samples\n",
    "            T3_all_Num = tf.reduce_sum(tf.cast(y_T3, tf.float32))\n",
    "\n",
    "            # Task 3 Accuracy\n",
    "            T3_accuracy = tf.divide(T3_Corrected_Num, T3_all_Num)\n",
    "            tf.summary.scalar('T3_accuracy', T3_accuracy)\n",
    "\n",
    "            T3_TP = T3_Corrected_Num\n",
    "            T3_TN = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T3), tf.math.logical_not(prediction_T3)), tf.float32))\n",
    "            T3_FP = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T3), prediction_T3), tf.float32))\n",
    "            T3_FN = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T3, tf.math.logical_not(prediction_T3)), tf.float32))\n",
    "\n",
    "            with tf.name_scope(\"T3_Precision\"):\n",
    "                T3_Precision = T3_TP / (T3_TP + T3_FP)\n",
    "                tf.summary.scalar('T3_Precision', T3_Precision)\n",
    "\n",
    "            with tf.name_scope(\"T3_Recall\"):\n",
    "                T3_Recall = T3_TP / (T3_TP + T3_FN)\n",
    "                tf.summary.scalar('T3_Recall', T3_Recall)\n",
    "\n",
    "            with tf.name_scope(\"T3_F_Score\"):\n",
    "                T3_F_Score = (2*T3_Precision*T3_Recall)/(T3_Precision+T3_Recall)\n",
    "                tf.summary.scalar('T3_F_Score', T3_F_Score)\n",
    "\n",
    "        # Task 4 Accuracy\n",
    "        with tf.name_scope('T4_accuracy'):\n",
    "            # Number of Classified Correctly\n",
    "            y_T4 = tf.equal(tf.argmax(y, 1), 3)\n",
    "            prediction_T4 = tf.equal(tf.argmax(prediction, 1), 3)\n",
    "            T4_Corrected_Num = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T4, prediction_T4), tf.float32))\n",
    "\n",
    "            # Number of All the Test Samples\n",
    "            T4_all_Num = tf.reduce_sum(tf.cast(y_T4, tf.float32))\n",
    "\n",
    "            # Task 4 Accuracy\n",
    "            T4_accuracy = tf.divide(T4_Corrected_Num, T4_all_Num)\n",
    "            tf.summary.scalar('T4_accuracy', T4_accuracy)\n",
    "\n",
    "            T4_TP = T4_Corrected_Num\n",
    "            T4_TN = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T4), tf.math.logical_not(prediction_T4)), tf.float32))\n",
    "            T4_FP = tf.reduce_sum(tf.cast(tf.math.logical_and(tf.math.logical_not(y_T4), prediction_T4), tf.float32))\n",
    "            T4_FN = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T4, tf.math.logical_not(prediction_T4)), tf.float32))\n",
    "\n",
    "            with tf.name_scope(\"T4_Precision\"):\n",
    "                T4_Precision = T4_TP / (T4_TP + T4_FP)\n",
    "                tf.summary.scalar('T4_Precision', T4_Precision)\n",
    "\n",
    "            with tf.name_scope(\"T4_Recall\"):\n",
    "                T4_Recall = T4_TP / (T4_TP + T4_FN)\n",
    "                tf.summary.scalar('T4_Recall', T4_Recall)\n",
    "\n",
    "            with tf.name_scope(\"T4_F_Score\"):\n",
    "                T4_F_Score = (2*T4_Precision*T4_Recall)/(T4_Precision+T4_Recall)\n",
    "                tf.summary.scalar('T4_F_Score', T4_F_Score)\n",
    "\n",
    "    # Calculate the Confusion Matrix\n",
    "    with tf.name_scope(\"Confusion_Matrix\"):\n",
    "        with tf.name_scope(\"T1_Label\"):\n",
    "            T1_T1 = T1_Corrected_Num\n",
    "            T1_T2 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T1, prediction_T2), tf.float32))\n",
    "            T1_T3 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T1, prediction_T3), tf.float32))\n",
    "            T1_T4 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T1, prediction_T4), tf.float32))\n",
    "\n",
    "            T1_T1_percent = tf.divide(T1_T1, T1_all_Num)\n",
    "            T1_T2_percent = tf.divide(T1_T2, T1_all_Num)\n",
    "            T1_T3_percent = tf.divide(T1_T3, T1_all_Num)\n",
    "            T1_T4_percent = tf.divide(T1_T4, T1_all_Num)\n",
    "\n",
    "            tf.summary.scalar('T1_T1_percent', T1_T1_percent)\n",
    "            tf.summary.scalar('T1_T2_percent', T1_T2_percent)\n",
    "            tf.summary.scalar('T1_T3_percent', T1_T3_percent)\n",
    "            tf.summary.scalar('T1_T4_percent', T1_T4_percent)\n",
    "\n",
    "        with tf.name_scope(\"T2_Label\"):\n",
    "            T2_T1 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T2, prediction_T1), tf.float32))\n",
    "            T2_T2 = T2_Corrected_Num\n",
    "            T2_T3 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T2, prediction_T3), tf.float32))\n",
    "            T2_T4 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T2, prediction_T4), tf.float32))\n",
    "\n",
    "            T2_T1_percent = tf.divide(T2_T1, T2_all_Num)\n",
    "            T2_T2_percent = tf.divide(T2_T2, T2_all_Num)\n",
    "            T2_T3_percent = tf.divide(T2_T3, T2_all_Num)\n",
    "            T2_T4_percent = tf.divide(T2_T4, T2_all_Num)\n",
    "\n",
    "            tf.summary.scalar('T2_T1_percent', T2_T1_percent)\n",
    "            tf.summary.scalar('T2_T2_percent', T2_T2_percent)\n",
    "            tf.summary.scalar('T2_T3_percent', T2_T3_percent)\n",
    "            tf.summary.scalar('T2_T4_percent', T2_T4_percent)\n",
    "\n",
    "        with tf.name_scope(\"T3_Label\"):\n",
    "            T3_T1 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T3, prediction_T1), tf.float32))\n",
    "            T3_T2 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T3, prediction_T2), tf.float32))\n",
    "            T3_T3 = T3_Corrected_Num\n",
    "            T3_T4 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T3, prediction_T4), tf.float32))\n",
    "\n",
    "            T3_T1_percent = tf.divide(T3_T1, T3_all_Num)\n",
    "            T3_T2_percent = tf.divide(T3_T2, T3_all_Num)\n",
    "            T3_T3_percent = tf.divide(T3_T3, T3_all_Num)\n",
    "            T3_T4_percent = tf.divide(T3_T4, T3_all_Num)\n",
    "\n",
    "            tf.summary.scalar('T3_T1_percent', T3_T1_percent)\n",
    "            tf.summary.scalar('T3_T2_percent', T3_T2_percent)\n",
    "            tf.summary.scalar('T3_T3_percent', T3_T3_percent)\n",
    "            tf.summary.scalar('T3_T4_percent', T3_T4_percent)\n",
    "\n",
    "        with tf.name_scope(\"T4_Label\"):\n",
    "            T4_T1 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T4, prediction_T1), tf.float32))\n",
    "            T4_T2 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T4, prediction_T2), tf.float32))\n",
    "            T4_T3 = tf.reduce_sum(tf.cast(tf.math.logical_and(y_T4, prediction_T3), tf.float32))\n",
    "            T4_T4 = T4_Corrected_Num\n",
    "\n",
    "            T4_T1_percent = tf.divide(T4_T1, T4_all_Num)\n",
    "            T4_T2_percent = tf.divide(T4_T2, T4_all_Num)\n",
    "            T4_T3_percent = tf.divide(T4_T3, T4_all_Num)\n",
    "            T4_T4_percent = tf.divide(T4_T4, T4_all_Num)\n",
    "\n",
    "            tf.summary.scalar('T4_T1_percent', T4_T1_percent)\n",
    "            tf.summary.scalar('T4_T2_percent', T4_T2_percent)\n",
    "            tf.summary.scalar('T4_T3_percent', T4_T3_percent)\n",
    "            tf.summary.scalar('T4_T4_percent', T4_T4_percent)\n",
    "\n",
    "    with tf.name_scope('Global_Evalution_Metrics'):\n",
    "        # Global Average Accuracy - Simple Algorithm\n",
    "        with tf.name_scope('Global_Average_Accuracy'):\n",
    "            correct_prediction = tf.equal(tf.argmax(prediction, 1), tf.argmax(y, 1))\n",
    "            Global_Average_Accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "            tf.summary.scalar('Global_Average_Accuracy', Global_Average_Accuracy)\n",
    "\n",
    "        with tf.name_scope('Kappa_Metric'):\n",
    "            Test_Set_Num = T1_all_Num + T2_all_Num + T3_all_Num + T4_all_Num\n",
    "\n",
    "            Actual_T1 = T1_all_Num\n",
    "            Actual_T2 = T2_all_Num\n",
    "            Actual_T3 = T3_all_Num\n",
    "            Actual_T4 = T4_all_Num\n",
    "\n",
    "            Prediction_T1 = T1_T1 + T2_T1 + T3_T1 + T4_T1\n",
    "            Prediction_T2 = T1_T2 + T2_T2 + T3_T2 + T4_T2\n",
    "            Prediction_T3 = T1_T3 + T2_T3 + T3_T3 + T4_T3\n",
    "            Prediction_T4 = T1_T4 + T2_T4 + T3_T4 + T4_T4\n",
    "\n",
    "            p0 = (T1_T1 + T2_T2 + T3_T3 + T4_T4) / Test_Set_Num\n",
    "            pe = (Actual_T1*Prediction_T1 + Actual_T2*Prediction_T2 + Actual_T3*Prediction_T3 + Actual_T4*Prediction_T4) / \\\n",
    "                 (Test_Set_Num*Test_Set_Num)\n",
    "\n",
    "            Kappa_Metric = (p0 - pe) / (1 - pe)\n",
    "            tf.summary.scalar('Kappa_Metric', Kappa_Metric)\n",
    "\n",
    "        with tf.name_scope('Micro_Averaged_Evalution'):\n",
    "            with tf.name_scope(\"Micro_Averaged_Confusion_Matrix\"):\n",
    "                TP_all = T1_TP + T2_TP + T3_TP + T4_TP\n",
    "                TN_all = T1_TN + T2_TN + T3_TN + T4_TN\n",
    "                FP_all = T1_FP + T2_FP + T3_FP + T4_FP\n",
    "                FN_all = T1_FN + T2_FN + T3_FN + T4_FN\n",
    "\n",
    "            with tf.name_scope(\"Micro_Global_Precision\"):\n",
    "                Micro_Global_Precision = TP_all / (TP_all + FP_all)\n",
    "                tf.summary.scalar('Micro_Global_Precision', Micro_Global_Precision)\n",
    "\n",
    "            with tf.name_scope(\"Micro_Global_Recall\"):\n",
    "                Micro_Global_Recall = TP_all / (TP_all + FN_all)\n",
    "                tf.summary.scalar('Micro_Global_Recall', Micro_Global_Recall)\n",
    "\n",
    "            with tf.name_scope(\"Micro_Global_F1_Score\"):\n",
    "                Micro_Global_F1_Score = (2*Micro_Global_Precision*Micro_Global_Recall)/(Micro_Global_Precision+Micro_Global_Recall)\n",
    "                tf.summary.scalar('Micro_Global_F1_Score', Micro_Global_F1_Score)\n",
    "\n",
    "        with tf.name_scope('Macro_Averaged_Evalution'):\n",
    "            with tf.name_scope(\"Macro_Global_Precision\"):\n",
    "                Macro_Global_Precision = (T1_Precision + T2_Precision + T3_Precision + T4_Precision) / 4\n",
    "                tf.summary.scalar('Macro_Global_Precision', Macro_Global_Precision)\n",
    "\n",
    "            with tf.name_scope(\"Macro_Global_Recall\"):\n",
    "                Macro_Global_Recall = (T1_Recall + T2_Recall + T3_Recall + T4_Recall) / 4\n",
    "                tf.summary.scalar('Macro_Global_Recall', Macro_Global_Recall)\n",
    "\n",
    "            with tf.name_scope(\"Macro_Global_F1_Score\"):\n",
    "                Macro_Global_F1_Score = (T1_F_Score + T2_F_Score + T3_F_Score + T4_F_Score) / 4\n",
    "                tf.summary.scalar('Macro_Global_F1_Score', Macro_Global_F1_Score)\n",
    "\n",
    "# Merge all the summaries\n",
    "merged = tf.summary.merge_all()\n",
    "\n",
    "# Initialize all the variables\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Start a saver to save the trained model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Summary the Training and Test Processing\n",
    "train_writer = tf.summary.FileWriter(SAVE + 'train_Writer', sess.graph)\n",
    "test_writer  = tf.summary.FileWriter(SAVE + 'test_Writer')\n",
    "\n",
    "for epoch in range(2019):\n",
    "    for batch_index in range(n_batch):\n",
    "        random_batch = random.sample(range(train_data.shape[0]), batch_size)\n",
    "        batch_xs = train_data[random_batch]\n",
    "        batch_ys = train_labels[random_batch]\n",
    "        sess.run(train_step, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 0.50})\n",
    "\n",
    "    # Accuracy on Training Set\n",
    "    train_acc, train_loss = sess.run([Global_Average_Accuracy, loss], feed_dict={x: train_data, y: train_labels, keep_prob: 1.0})\n",
    "\n",
    "    # Accuracy on Test Set\n",
    "    test_summary, test_acc, test_loss = sess.run([merged, Global_Average_Accuracy, loss], feed_dict={x: test_data, y: test_labels, keep_prob: 1.0})\n",
    "    test_writer.add_summary(test_summary, epoch)\n",
    "    \n",
    "    # Show the Model Capability\n",
    "    print(\"Iter \" + str(epoch) + \", Training Accuracy: \" + str(train_acc) + \", Testing Accuracy: \" + str(test_acc))\n",
    "\n",
    "    # Save the Model Every 100 Epoches\n",
    "    if epoch % 100 == 0:\n",
    "        saver.save(sess, save_path=SAVE + 'Model_Saver/Ite_%s' % epoch)\n",
    "\n",
    "    if epoch == 2001:\n",
    "        output_prediction = sess.run(prediction, feed_dict={x: test_data, y: test_labels, keep_prob: 1.0})\n",
    "        np.savetxt(SAVE + \"prediction.csv\", output_prediction, delimiter=\",\")\n",
    "        np.savetxt(SAVE + \"labels.csv\", test_labels, delimiter=\",\")\n",
    "\n",
    "train_writer.close()\n",
    "test_writer.close()\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-9b9a68c63402>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenfromtxt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_label\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\npyio.py\u001b[0m in \u001b[0;36mgenfromtxt\u001b[1;34m(fname, dtype, comments, delimiter, skip_header, skip_footer, converters, missing_values, filling_values, usecols, names, excludelist, deletechars, replace_space, autostrip, case_sensitive, defaultfmt, unpack, usemask, loose, invalid_raise, max_rows, encoding)\u001b[0m\n\u001b[0;32m   1780\u001b[0m                         first_line = (\n\u001b[0;32m   1781\u001b[0m                             ''.join(first_line.split(comments)[1:]))\n\u001b[1;32m-> 1782\u001b[1;33m                 \u001b[0mfirst_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfirst_line\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1783\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1784\u001b[0m             \u001b[1;31m# return an empty array if the datafile is empty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\_iotools.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handyman\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_decode_line\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    227\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\numpy\\lib\\_iotools.py\u001b[0m in \u001b[0;36m_delimited_splitter\u001b[1;34m(self, line)\u001b[0m\n\u001b[0;32m    199\u001b[0m         \u001b[1;34m\"\"\"Chop off comments, strip, and split at delimiter. \"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m             \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcomments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[0mline\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\" \\r\\n\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'split'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<module 'pandas' from 'C:\\\\Users\\\\USER\\\\anaconda3\\\\lib\\\\site-packages\\\\pandas\\\\__init__.py'>\n"
     ]
    }
   ],
   "source": [
    "print(pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[-0.9930175 , -0.9617446 , -0.925241  , ..., -0.690498  ,\n",
       "         -0.7249668 , -0.75864524],\n",
       "        [-0.3833839 , -0.30240378, -0.23623629, ...,  2.2497506 ,\n",
       "          1.9954233 ,  1.6994805 ],\n",
       "        [-0.4558134 , -0.44716206, -0.43938726, ..., -0.282473  ,\n",
       "         -0.2673757 , -0.25617656],\n",
       "        ...,\n",
       "        [ 0.02301265,  0.22459418,  0.43343523, ...,  0.12652127,\n",
       "          0.10852579,  0.08213207],\n",
       "        [ 0.4945519 ,  0.6434237 ,  0.7546663 , ...,  1.8252416 ,\n",
       "          1.4504727 ,  1.0697433 ],\n",
       "        [-0.8527063 , -0.81998396, -0.7856229 , ..., -0.96927   ,\n",
       "         -0.9786318 , -0.9874144 ]]], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(batch_xs, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[2],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [3],\n",
       "        [0],\n",
       "        [3],\n",
       "        [1],\n",
       "        [0],\n",
       "        [3],\n",
       "        [0],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [3],\n",
       "        [1],\n",
       "        [3],\n",
       "        [0],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [1],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [2],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [3],\n",
       "        [1],\n",
       "        [0],\n",
       "        [3],\n",
       "        [2],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [3],\n",
       "        [2],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [3],\n",
       "        [0],\n",
       "        [2],\n",
       "        [0],\n",
       "        [3],\n",
       "        [1],\n",
       "        [2],\n",
       "        [1],\n",
       "        [0],\n",
       "        [1],\n",
       "        [3],\n",
       "        [3],\n",
       "        [0],\n",
       "        [3],\n",
       "        [2],\n",
       "        [0],\n",
       "        [1]]], dtype=int64)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.expand_dims(batch_ys, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-72-575f178ee557>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-72-575f178ee557>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    tensorboard --logdir logs\u001b[0m\n\u001b[1;37m                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7991b78e80db9f7e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7991b78e80db9f7e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 13716), started 0:02:46 ago. (Use '!kill 13716' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-3e1a0fc8d115a4b7\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-3e1a0fc8d115a4b7\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'FileWriter' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-76-5532910629c0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrain_writer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'FileWriter' object is not callable"
     ]
    }
   ],
   "source": [
    "train_writer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
